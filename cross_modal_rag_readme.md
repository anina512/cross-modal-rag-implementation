# Cross-Modal RAG â€” Multimodal Misinformation Detection  
**Retrieve + Reason over Text & Images using CLIP, SBERT, FAISS & LLMs**

This project implements a **Cross-Modal Retrieval-Augmented Generation (RAG)** system that takes a **claim + image pair**, retrieves the most relevant textual and visual evidence, builds an evidence-aware prompt, and sends it to an LLM to determine whether the claim is:
- **True**
- **False**
- **Uncertain**

---

## ğŸ” Core Idea

We don't rely on the claim text alone.

We:
1. Embed **all text** with SBERT  
2. Embed **all images** with CLIP  
3. Build two FAISS indexes  
4. Retrieve:
   - Top-K similar texts  
   - Top-K similar images  
   - Cross-modal image matches  
5. Build a fused RAG prompt  
6. Send to an LLM for reasoning  

---

## ğŸ§  Architecture

```
(Claim + Image)
â”‚
â–¼
[Cross-Modal Retriever]
â”‚         â”‚         â”‚
Textâ†’Text  Imgâ†’Img  Textâ†’Img
â–¼         â–¼         â–¼
Top-K Evidence Samples
â”‚
â–¼
[Prompt Builder]
â”‚
â–¼
LLM
â”‚
â–¼
True / False / Uncertain
```

---

## ğŸ“‚ Project Structure

```
cross-modal-rag/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ cache_train.csv
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ ids.npy
â”‚   â”œâ”€â”€ text_embs.npy
â”‚   â””â”€â”€ image_embs.npy
â”‚
â”œâ”€â”€ indexes/
â”‚   â”œâ”€â”€ text.index
â”‚   â””â”€â”€ image.index
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ build_embeddings.py
â”‚   â”œâ”€â”€ build_index.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”œâ”€â”€ llm_inference.py
â”‚   â”œâ”€â”€ test_data_loading.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_rag_prompt.py
â”‚
â””â”€â”€ requirements.txt
```

---

## ğŸ›  Installation

```bash
git clone <repo>
cd cross-modal-rag
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš¦ Step 1 â€” Load Dataset & Download Images

```bash
python -m src.test_data_loading
```

This:
- downloads image+text pairs
- saves them to `data/images/`
- caches metadata in `data/cache_train.csv`

---

## ğŸ”¡ Step 2 â€” Generate Embeddings

```bash
python -m src.build_embeddings
```

Produces:
```
embeddings/ids.npy
embeddings/text_embs.npy
embeddings/image_embs.npy
```

---

## ğŸ“¦ Step 3 â€” Build FAISS Indexes

```bash
python -m src.build_index
```

Outputs:
```
indexes/text.index
indexes/image.index
```

---

## ğŸ” Step 4 â€” Test Retrieval

```bash
python -m src.test_retrieval
```

Validates:
- Text â†’ Text search
- Image â†’ Image similarity
- Text â†’ Image via CLIP

---

## ğŸ¤– Step 5 â€” Run Full RAG

```bash
python -m src.test_rag_prompt
```

This will:
1. Retrieve multimodal evidence
2. Construct a fact-checking prompt
3. Send to LLM
4. Return a verdict + explanation

Example:
```
Verdict: False  
Explanation: Retrieved evidence contradicts the claim.
```

---

## ğŸ”Œ LLM Model Switching

Edit here:
```bash
src/llm_inference.py
```

Change:
```python
model_name = "microsoft/phi-2"
```

to:
```python
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
# or
model_name = "microsoft/Phi-3-mini-4k-instruct"
# or any HF model
```

---

## ğŸ§© Integrating Your Fine-Tuned Model

Once your custom LLM is trained:

1. Push it to HuggingFace OR
2. Load it locally:

```python
model = AutoModelForCausalLM.from_pretrained("./my_model")
```

Everything else stays the same.

---

## ğŸ“Œ Requirements

- Python 3.9+
- PyTorch (+ CUDA)
- SentenceTransformers
- FAISS
- CLIP
- PIL
- Transformers
- Accelerate

---

## â• Extensions

You can extend this to:
- Larger datasets (9k â†’ 100k+)
- Add image captions (BLIP / LLaVA)
- Use GPT-4V or LLaVA-Next
- Run via FastAPI / Streamlit / Gradio

---

## ğŸ“„ License

MIT License

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“§ Contact

For questions or collaboration opportunities, please open an issue on GitHub.
